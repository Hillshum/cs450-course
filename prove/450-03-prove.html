<!DOCTYPE html>
<html>

<head>
    <title>CS 450 - Prove</title>
    <link rel="stylesheet" type="text/css" href="../course/style.css" />
</head>

<body>
    <div id= "main" class="splash">

<!--         <div id="header">
            <img class="banner" alt="CS 450 Banner" title="CS 450 Banner" src="../course/cs450.jpg" />
        </div>
 -->
        <article>

            <h1>03 Prove : Assignment - Decision Tree Classifier</h1>

            <h2>Objective</h2>
            <p>Understand the ID3 Decision Tree algorithm.</p>
            <p>Please note that you have 2 weeks to implement and experiment with this algorithm. This is because it can be a challenge! Do not put this off until next week. You should try to complete the coding portion this week to leave next week for experimentation.</p>
            <h2>Instructions</h2>
            <p>Add to your experiment shell from the previous assignment. Implement a new algorithm, the ID3 Decision Tree.</p>
            <h2>Experiment Guidelines</h2>
            <p>After implementing the algorithm, use it to classify the following datasets:</p>
            <ol>
                <li><a href="https://archive.ics.uci.edu/ml/datasets/Iris" target="_blank" title="Iris Dataset">Iris</a> (our old friend!)</li>
                <li><a href="https://archive.ics.uci.edu/ml/datasets/Lenses" target="_blank" title="Lenses dataset">Lenses</a><!-- &nbsp;(can also be found in the Weka datasets directory as: contact-lenses.arff) --></li>
                <li><a href="https://archive.ics.uci.edu/ml/machine-learning-databases/voting-records/" target="_blank" title="Voting">Voting</a> - Please note that you are trying to predict the political party which is listed as the first column (not last) in the data file.<!-- &nbsp;(can also be found in the Weka datasets directory as: vote.arff) --></li>
            </ol>
            <p>Optional:</p>
            <ol>
                <li><a href="https://archive.ics.uci.edu/ml/datasets/Credit+Approval" target="_blank" title="Credit Screening">Credit Screening</a></li>
                <li><a href="https://archive.ics.uci.edu/ml/datasets/Chess+%28King-Rook+vs.+King%29" target="_blank" title="Chess Dataset">Chess</a> (King-pawn vs. King)</li>
            </ol>
            <p>For each one, experiment with different parameters for the algorithm.</p>
            <p>For each dataset, you should produce some text version of the decision tree that is induced by your algorithm (which will be submitted in your submission file).</p>
            <p>In addition, for each one, compare your implementation of the ID3 algorithm to an existing one (e.g., scikit-learn, Weka) and compare/contrast the results.</p>
            <h2>Questions for Consideration</h2>
            <ul>
                <li>How should numerical data be handled?</li>
                <li>How should missing data be handled?</li>
                <li>Would pruning be helpful? If so, what approach would you use?</li>
            </ul>
            <h2>Requirements</h2>
            <p>As always, you are encouraged to go above and beyond and take initiative in your learning. As described in the syllabus, meeting the minimum standard requirements can qualify you for 93%, but going above and beyond is necessary to get 100%. The following are the expectations for a minimum standard, with a few suggestions for going above and beyond.</p>
            <h3>Minimum Standard Requirements</h3>
            <ul>
                <li>Implement the basic ID3 decision tree algorithm</li>
                <li>Handle nominal and numeric data (if you choose, numeric data can be handled via pre-processing)</li>
                <li>Handle missing data</li>
                <li>Produce a textual view of your resulting tree</li>
                <li>Basic experimentation</li>
                <ul>
                    <li>Use the supplied datasets</li>
                    <li>Compare to existing implementations</li>
                </ul>
            </ul>
            <h3>Opportunities to go above and beyond:</h3>
            <ul>
                <li>Explore additional approaches to handle numeric data and/or missing data (e.g., effectiveness of different sizes and boundaries of bins as shown on different data sets; incorporating into the algorithm itself)</li>
                <li>Pruning</li>
                <li>Experiment on many more datasets (e.g., how does the algorithm behave as the number of instances and/or attributes changes dramatically?)</li>
                <li>More robust experiements, such as using many more parameters, configurations, 10-fold cross validation, etc.</li>
                <li>Implement a technique to handle splitting on multiple attributes in the same node</li>
                <li>Regression</li>
                <li>Any other ideas you have</li>
            </ul>
            <h2>Submission</h2>
            <p>When complete, push your code to a public GitHub repository and answer the questions in the <a href="../forms/450-03-prove-form.txt">submission text file</a>. Please fill it out and upload it to I-Learn.</p>

        </article>
    </div>


</body>

</html>